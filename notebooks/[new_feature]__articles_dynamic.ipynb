{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/')\n",
    "\n",
    "from scripts.utils import create_one_hot_encoding\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_parquet('../../data/compressed_dataset/transactions.parquet')\n",
    "df_articles = pd.read_parquet('../../data/compressed_dataset/articles.parquet')\n",
    "df_customers = pd.read_parquet('../../data/compressed_dataset/customers.parquet')\n",
    "\n",
    "article_id_int = pd.read_pickle('../../data/compressed_dataset/article_id_int.pickle')\n",
    "int_article_id = pd.read_pickle('../../data/compressed_dataset/int_article_id.pickle')\n",
    "\n",
    "customer_id_int = pd.read_pickle('../../data/compressed_dataset/customer_id_int.pickle')\n",
    "int_customer_id = pd.read_pickle('../../data/compressed_dataset/int_customer_id.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Days from first purchase, date from last purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions['t_dat'] = pd.to_datetime(df_transactions['t_dat'])\n",
    "last_trans_date = df_transactions['t_dat'].max()\n",
    "\n",
    "\n",
    "num_train_weeks = 20\n",
    "\n",
    "for i in range(num_train_weeks-1, -1, -1):\n",
    "    \n",
    "    candidates_predictions = pd.read_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/ranker_train_set_utils/models_preds_{}.parquet'.format(i))['article_id'].unique()\n",
    "        \n",
    "    \n",
    "    last_local_date = last_trans_date - timedelta(days=i * 7)\n",
    "    \n",
    "    train_set_st1 = df_transactions[df_transactions['t_dat'] <= last_local_date].copy()\n",
    "    train_set_st1['weeks_before_sub'] = i\n",
    "    \n",
    "    train_set_st1 = train_set_st1[train_set_st1['article_id'].isin(candidates_predictions)]\n",
    "        \n",
    "    df_trans_first_purchase = train_set_st1.groupby(['article_id']).first().reset_index()\n",
    "    df_trans_last_purchase = train_set_st1.groupby(['article_id']).last().reset_index()\n",
    "    \n",
    "    train_set_st1 = train_set_st1.groupby(['article_id']).first().reset_index()\n",
    "    \n",
    "    train_set_st1 = train_set_st1.merge(df_trans_first_purchase[['article_id', 't_dat']].rename(columns={'t_dat': 'first_dat'}),\n",
    "                                      on=['article_id'])\n",
    "\n",
    "    train_set_st1 = train_set_st1.merge(df_trans_last_purchase[['article_id', 't_dat']].rename(columns={'t_dat': 'last_dat'}),\n",
    "                                          on=['article_id'])\n",
    "    \n",
    "    \n",
    "    train_set_st1['article__num_days_from_first_purchase'] = (last_local_date - train_set_st1['first_dat']).dt.days\n",
    "    train_set_st1['article__num_days_from_last_purchase'] = (last_local_date - train_set_st1['last_dat']).dt.days\n",
    "    \n",
    "    train_set_st1 = train_set_st1[['article_id', 'weeks_before_sub',\n",
    "                                   'article__num_days_from_first_purchase', 'article__num_days_from_last_purchase']]\n",
    "    \n",
    "    if i == num_train_weeks-1:\n",
    "        all_train_set = train_set_st1\n",
    "    else:\n",
    "        all_train_set = pd.concat([all_train_set, train_set_st1], ignore_index=True)\n",
    "    \n",
    "\n",
    "all_train_set.to_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/features/articles/dynamic/num_days_from_purchases.parquet',\n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean_sales_channel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions['t_dat'] = pd.to_datetime(df_transactions['t_dat'])\n",
    "last_trans_date = df_transactions['t_dat'].max()\n",
    "\n",
    "\n",
    "num_train_weeks = 20\n",
    "\n",
    "for i in range(num_train_weeks-1, -1, -1):\n",
    "    \n",
    "    candidates_predictions = pd.read_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/ranker_train_set_utils/models_preds_{}.parquet'.format(i))['article_id'].unique()\n",
    "        \n",
    "    last_local_date = last_trans_date - timedelta(days=i * 7)\n",
    "    \n",
    "    train_set_st1 = df_transactions[df_transactions['t_dat'] <= last_local_date].copy()\n",
    "    \n",
    "    train_set_st1 = train_set_st1[train_set_st1['article_id'].isin(candidates_predictions)]\n",
    "        \n",
    "    train_set_st1 = train_set_st1.groupby(['article_id'])['sales_channel_id'].mean().reset_index().rename(columns={'sales_channel_id': 'article__mean_sales_channel_id'})\n",
    "    train_set_st1['weeks_before_sub'] = i\n",
    "        \n",
    "    train_set_st1 = train_set_st1[['article_id', 'weeks_before_sub',\n",
    "                                   'article__mean_sales_channel_id']]\n",
    "    \n",
    "    if i == num_train_weeks-1:\n",
    "        all_train_set = train_set_st1\n",
    "    else:\n",
    "        all_train_set = pd.concat([all_train_set, train_set_st1], ignore_index=True)\n",
    "    \n",
    "\n",
    "all_train_set.to_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/features/articles/dynamic/mean_sales_channel_id.parquet',\n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mean_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions['t_dat'] = pd.to_datetime(df_transactions['t_dat'])\n",
    "last_trans_date = df_transactions['t_dat'].max()\n",
    "\n",
    "\n",
    "num_train_weeks = 20\n",
    "\n",
    "for i in range(num_train_weeks-1, -1, -1):\n",
    "    \n",
    "    candidates_predictions = pd.read_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/ranker_train_set_utils/models_preds_{}.parquet'.format(i))['article_id'].unique()\n",
    "        \n",
    "    \n",
    "    last_local_date = last_trans_date - timedelta(days=i * 7)\n",
    "    \n",
    "    train_set_st1 = df_transactions[df_transactions['t_dat'] <= last_local_date].copy()\n",
    "    \n",
    "    train_set_st1 = train_set_st1[train_set_st1['article_id'].isin(candidates_predictions)]\n",
    "    \n",
    "    train_set_last = train_set_st1.groupby(['article_id'])['price'].last().reset_index().rename(columns={'price': 'article__last_price'})\n",
    "    \n",
    "    train_set_st1 = train_set_st1.groupby(['article_id'])['price'].mean().reset_index().rename(columns={'price': 'article__mean_price'})\n",
    "        \n",
    "    train_set_st1['weeks_before_sub'] = i\n",
    "        \n",
    "    train_set_st1 = train_set_st1[['article_id', 'weeks_before_sub',\n",
    "                                   'article__mean_price']]\n",
    "    \n",
    "    train_set_st1 = train_set_st1.merge(train_set_last[['article_id', 'article__last_price']], how='left', on=['article_id'])\n",
    "    train_set_st1['article__last_price_ratio'] = train_set_st1['article__last_price'] / train_set_st1['article__mean_price']\n",
    "    \n",
    "    if i == num_train_weeks-1:\n",
    "        all_train_set = train_set_st1\n",
    "    else:\n",
    "        all_train_set = pd.concat([all_train_set, train_set_st1], ignore_index=True)\n",
    "    \n",
    "\n",
    "all_train_set.to_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/features/articles/dynamic/price.parquet',\n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# num purchased for all time / last 30d / last 7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions['t_dat'] = pd.to_datetime(df_transactions['t_dat'])\n",
    "last_trans_date = df_transactions['t_dat'].max()\n",
    "\n",
    "\n",
    "num_train_weeks = 20\n",
    "\n",
    "for i in range(num_train_weeks-1, -1, -1):\n",
    "    \n",
    "    candidates_predictions = pd.read_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/ranker_train_set_utils/models_preds_{}.parquet'.format(i))['article_id'].unique()\n",
    "        \n",
    "    \n",
    "    last_local_date = last_trans_date - timedelta(days=i * 7)\n",
    "    \n",
    "    train_set_st1 = df_transactions[df_transactions['t_dat'] <= last_local_date].copy()\n",
    "    \n",
    "    train_set_st1 = train_set_st1[train_set_st1['article_id'].isin(candidates_predictions)]\n",
    "        \n",
    "    train_set_st1_temp = train_set_st1.groupby(['article_id'])['customer_id'].count().reset_index().rename(columns={'customer_id': 'article__num_purchased_customers'})\n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1.groupby(['article_id'])['customer_id'].nunique().reset_index().rename(columns={'customer_id': 'article__num_unique_purchased_customers'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    \n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] >= last_local_date-timedelta(days=90)].groupby(['article_id'])['customer_id'].count().reset_index().rename(columns={'customer_id': 'article__num_purchased_customers_last90days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] >= last_local_date-timedelta(days=90)].groupby(['article_id'])['customer_id'].nunique().reset_index().rename(columns={'customer_id': 'article__num_unique_purchased_customers_last90days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    \n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] >= last_local_date-timedelta(days=30)].groupby(['article_id'])['customer_id'].count().reset_index().rename(columns={'customer_id': 'article__num_purchased_customers_last30days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] >= last_local_date-timedelta(days=30)].groupby(['article_id'])['customer_id'].nunique().reset_index().rename(columns={'customer_id': 'article__num_unique_purchased_customers_last30days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    \n",
    "    \n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] >= last_local_date-timedelta(days=7)].groupby(['article_id'])['customer_id'].count().reset_index().rename(columns={'customer_id': 'article__num_purchased_customers_last7days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] >= last_local_date-timedelta(days=7)].groupby(['article_id'])['customer_id'].nunique().reset_index().rename(columns={'customer_id': 'article__num_unique_purchased_customers_last7days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    \n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] == last_local_date-timedelta(days=7)].groupby(['article_id'])['customer_id'].count().reset_index().rename(columns={'customer_id': 'article__num_purchased_customers_last1days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    train_set_st1_temp = train_set_st1_temp.merge(train_set_st1[train_set_st1['t_dat'] == last_local_date-timedelta(days=7)].groupby(['article_id'])['customer_id'].nunique().reset_index().rename(columns={'customer_id': 'article__num_unique_purchased_customers_last1days'}),\n",
    "                                        on=['article_id'], how='left')\n",
    "    \n",
    "    train_set_st1_temp['article__unique_ratio'] = train_set_st1_temp['article__num_unique_purchased_customers'] / train_set_st1_temp['article__num_purchased_customers']\n",
    "    train_set_st1_temp['article__unique_ratio_last90days'] = train_set_st1_temp['article__num_unique_purchased_customers_last90days'] / train_set_st1_temp['article__num_purchased_customers_last90days']\n",
    "    train_set_st1_temp['article__unique_ratio_last30days'] = train_set_st1_temp['article__num_unique_purchased_customers_last30days'] / train_set_st1_temp['article__num_purchased_customers_last30days']\n",
    "    train_set_st1_temp['article__unique_ratio_last7days'] = train_set_st1_temp['article__num_unique_purchased_customers_last7days'] / train_set_st1_temp['article__num_purchased_customers_last7days']\n",
    "    train_set_st1_temp['article__unique_ratio_last1days'] = train_set_st1_temp['article__num_unique_purchased_customers_last1days'] / train_set_st1_temp['article__num_purchased_customers_last1days']\n",
    "\n",
    "    \n",
    "    train_set_st1_temp['weeks_before_sub'] = i\n",
    "        \n",
    "    \n",
    "        \n",
    "    train_set_st1_temp = train_set_st1_temp[['article_id', 'weeks_before_sub',\n",
    "                                   'article__num_purchased_customers', 'article__num_unique_purchased_customers',\n",
    "                                   'article__num_purchased_customers_last90days', 'article__num_unique_purchased_customers_last90days',\n",
    "                                   'article__num_purchased_customers_last30days', 'article__num_unique_purchased_customers_last30days',\n",
    "                                   'article__num_purchased_customers_last7days', 'article__num_unique_purchased_customers_last7days',\n",
    "                                   'article__num_purchased_customers_last1days', 'article__num_unique_purchased_customers_last1days',\n",
    "                                   'article__unique_ratio', 'article__unique_ratio_last30days', 'article__unique_ratio_last7days', 'article__unique_ratio_last1days'\n",
    "                                  ]]\n",
    "    \n",
    "    if i == num_train_weeks-1:\n",
    "        all_train_set = train_set_st1_temp\n",
    "    else:\n",
    "        all_train_set = pd.concat([all_train_set, train_set_st1_temp], ignore_index=True)\n",
    "    \n",
    "all_train_set.fillna(0, inplace=True)\n",
    "all_train_set.to_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/features/articles/dynamic/num_purchases__ratios.parquet',\n",
    "                        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = pd.read_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/features/articles/dynamic/num_days_from_purchases.parquet')\n",
    "num_purchases = pd.read_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/features/articles/dynamic/num_purchases__ratios.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = num_days.merge(num_purchases, how='left', on=['article_id', 'weeks_before_sub'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats['article__frequency_purchases'] = feats['article__num_days_from_first_purchase'] / feats['article__num_purchased_customers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats[['article_id', 'weeks_before_sub', 'article__frequency_purchases']].to_parquet('/home/juravlik/PycharmProjects/kaggle_hnm_recsys/data/features/articles/dynamic/article__frequency_purchases.parquet',\n",
    "                                                                                    index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_hnm_recsys",
   "language": "python",
   "name": "kaggle_hnm_recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
